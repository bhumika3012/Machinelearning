{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhumika3012/Machinelearning/blob/master/notebooks/langchain-13-min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "50dvxjqCFmhF",
        "outputId": "ac2667b2-a206-4942-80db-3d579ac0b5ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 <command> [options]\n",
            "\n",
            "Commands:\n",
            "  install                     Install packages.\n",
            "  download                    Download packages.\n",
            "  uninstall                   Uninstall packages.\n",
            "  freeze                      Output installed packages in requirements format.\n",
            "  inspect                     Inspect the python environment.\n",
            "  list                        List installed packages.\n",
            "  show                        Show information about installed packages.\n",
            "  check                       Verify installed packages have compatible dependencies.\n",
            "  config                      Manage local and global configuration.\n",
            "  search                      Search PyPI for packages.\n",
            "  cache                       Inspect and manage pip's wheel cache.\n",
            "  index                       Inspect information available from package indexes.\n",
            "  wheel                       Build wheels from your requirements.\n",
            "  hash                        Compute hashes of package archives.\n",
            "  completion                  A helper command used for command completion.\n",
            "  debug                       Show information useful for debugging.\n",
            "  help                        Show help for commands.\n",
            "\n",
            "General Options:\n",
            "  -h, --help                  Show help.\n",
            "  --debug                     Let unhandled exceptions propagate outside the main subroutine,\n",
            "                              instead of logging them to stderr.\n",
            "  --isolated                  Run pip in an isolated mode, ignoring environment variables and user\n",
            "                              configuration.\n",
            "  --require-virtualenv        Allow pip to only run in a virtual environment; exit with an error\n",
            "                              otherwise.\n",
            "  --python <python>           Run pip with the specified Python interpreter.\n",
            "  -v, --verbose               Give more output. Option is additive, and can be used up to 3 times.\n",
            "  -V, --version               Show version and exit.\n",
            "  -q, --quiet                 Give less output. Option is additive, and can be used up to 3 times\n",
            "                              (corresponding to WARNING, ERROR, and CRITICAL logging levels).\n",
            "  --log <path>                Path to a verbose appending log.\n",
            "  --no-input                  Disable prompting for input.\n",
            "  --keyring-provider <keyring_provider>\n",
            "                              Enable the credential lookup via the keyring library if user input\n",
            "                              is allowed. Specify which mechanism to use [disabled, import,\n",
            "                              subprocess]. (default: disabled)\n",
            "  --proxy <proxy>             Specify a proxy in the form\n",
            "                              scheme://[user:passwd@]proxy.server:port.\n",
            "  --retries <retries>         Maximum number of retries each connection should attempt (default 5\n",
            "                              times).\n",
            "  --timeout <sec>             Set the socket timeout (default 15 seconds).\n",
            "  --exists-action <action>    Default action when a path already exists: (s)witch, (i)gnore,\n",
            "                              (w)ipe, (b)ackup, (a)bort.\n",
            "  --trusted-host <hostname>   Mark this host or host:port pair as trusted, even though it does not\n",
            "                              have valid or any HTTPS.\n",
            "  --cert <path>               Path to PEM-encoded CA certificate bundle. If provided, overrides\n",
            "                              the default. See 'SSL Certificate Verification' in pip documentation\n",
            "                              for more information.\n",
            "  --client-cert <path>        Path to SSL client certificate, a single file containing the private\n",
            "                              key and the certificate in PEM format.\n",
            "  --cache-dir <dir>           Store the cache data in <dir>.\n",
            "  --no-cache-dir              Disable the cache.\n",
            "  --disable-pip-version-check\n",
            "                              Don't periodically check PyPI to determine whether a new version of\n",
            "                              pip is available for download. Implied with --no-index.\n",
            "  --no-color                  Suppress colored output.\n",
            "  --no-python-version-warning\n",
            "                              Silence deprecation warnings for upcoming unsupported Pythons.\n",
            "  --use-feature <feature>     Enable new functionality, that may be backward incompatible.\n",
            "  --use-deprecated <feature>  Enable deprecated functionality, that will be removed in the future.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-65b7fe188ad8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfind_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Load environment variables\n",
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "from dotenv import load_dotenv,find_dotenv\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "llm(\"explain large language models in one sentence\")"
      ],
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
        "\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "JtHgQ5XpJmgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert data scientist\"),\n",
        "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
        "]\n",
        "response=chat(messages)\n",
        "\n",
        "print(response.content,end='\\n')"
      ],
      "metadata": {
        "id": "yrfYfKfdJyyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import prompt and define PromptTemplate\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models.\n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "2grf7I8AJ_hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run LLM with PromptTemplate\n",
        "\n",
        "llm(prompt.format(concept=\"autoencoder\"))"
      ],
      "metadata": {
        "id": "vcz7Q9Y-KFvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import LLMChain and define chain with language model and prompt as arguments.\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"autoencoder\"))"
      ],
      "metadata": {
        "id": "dm78i-rUKXIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a second prompt\n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "B6MF4-nMKul3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"autoencoder\")\n",
        "print(explanation)"
      ],
      "metadata": {
        "id": "SkJKFyk1K-MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([explanation])\n"
      ],
      "metadata": {
        "id": "mDDu1B_SLQls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Individual text chunks can be accessed with \"page_content\"\n",
        "\n",
        "texts[0].page_content"
      ],
      "metadata": {
        "id": "F6lfAdeuLhtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and instantiate OpenAI embeddings\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
      ],
      "metadata": {
        "id": "Z5sv4e3tLw2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the first text chunk into a vector with the embedding\n",
        "\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(query_result)"
      ],
      "metadata": {
        "id": "dqzoir4hMlfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize Pinecone client\n",
        "\n",
        "import os\n",
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=os.getenv('PINECONE_API_KEY'),\n",
        "    environment=os.getenv('PINECONE_ENV')\n",
        ")"
      ],
      "metadata": {
        "id": "QaOY5bIZM3Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload vectors to Pinecone\n",
        "\n",
        "index_name = \"langchain-quickstart\"\n",
        "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "lZhSUt3FNBzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do a simple vector similarity search\n",
        "\n",
        "query = \"What is magical about an autoencoder?\"\n",
        "result = search.similarity_search(query)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "cCXVuXwPNKcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Python REPL tool and instantiate Python agent\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.llms.openai import OpenAI\n",
        "\n",
        "agent_executor = create_python_agent(\n",
        "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "4Ogz8luZNRnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the Python agent\n",
        "\n",
        "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")"
      ],
      "metadata": {
        "id": "BVHMDj0sNi09"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}